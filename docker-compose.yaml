# This docker-compose is for developer convenience, not for running in production.

volumes:

  shared-token:

services:

  postgres:  # for hive
    image: postgres:16.3
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive

  kafka:
    image: apache/kafka-native:4.0.0
    ports:
      - 9092:9092
    environment:
      # default config
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://localhost:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      
      # custom config
      KAFKA_DELETE_TOPIC_ENABLE: true

  minio:
    image: minio/minio:RELEASE.2025-02-07T23-21-09Z
    environment:
      MINIO_ROOT_USER: miniouser
      MINIO_ROOT_PASSWORD: miniopassword
    ports:
      - 9000:9000
      - 9001:9001
    command: 'server /data --console-address ":9001"'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 1s
      retries: 5

  minio-create-bucket:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_URL: http://minio:9000
      MINIO_USER: miniouser
      MINIO_PWD: miniopassword
      MINIO_CTS_USER: cts
      MINIO_CTS_PWD: ctspassword
    entrypoint: /s3_policies/minio_create_bucket_entrypoint.sh
    volumes:
      - ./docker_compose/s3_policies:/s3_policies

  mongodb:
    image: mongo:7.0.14
    ports:
      - 27017:27017
    # environment:
    #  - MONGO_INITDB_ROOT_USERNAME=root
    #  - MONGO_INITDB_ROOT_PASSWORD=secret

  auth2:
    image: ghcr.io/kbase/auth2:0.7.1
    platform: linux/amd64
    ports:
      - 8080:8080
    environment:
       mongo_host: "mongodb:27017"
       test_mode_enabled: "true"
       identity_providers: ""
    command:
      - "-template"
      - "/kb/deployment/conf/.templates/deployment.cfg.templ:/kb/deployment/conf/deployment.cfg"
      - "/kb/deployment/bin/start_auth2.sh"
    depends_on:
      - mongodb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 1s
      # https://github.com/kbase/auth2/issues/443
      retries: 30

  create_auth2_user:
    build:  # yuck. If there's a better way to do this I'm all ears
      context: ./docker_compose/create_auth_user
      dockerfile: Dockerfile
    depends_on:
      auth2:
        condition: service_healthy
    environment:
      AUTH_URL: http://auth2:8080
      USER_NAME: cdm_event_processor
      HAS_NERSC_ACCOUNT_ROLE: HAS_NERSC_ACCOUNT
      KBASE_STAFF_ROLE: KBASE_STAFF
      KBASE_AUTH2_ADMIN_ROLE: CDM_TASK_SERVICE_ADMIN
      TOKEN_OUTFILE: /shared/token.txt
    volumes:
      - shared-token:/shared

  spark-master:
    image: ghcr.io/kbase/cdm-spark-standalone:pr-29
    platform: linux/amd64
    ports:
      - 8090:8090
    environment:
      SPARK_MODE: master
      SPARK_MASTER_WEBUI_PORT: 8090
      MAX_EXECUTORS: 4
      EXECUTOR_CORES: 2
      MAX_CORES_PER_APPLICATION: 10
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive
      POSTGRES_URL: postgres:5432
      DATANUCLEUS_AUTO_CREATE_TABLES: true

  spark-worker-1:
    image: ghcr.io/kbase/cdm-spark-standalone:pr-29
    platform: linux/amd64
    depends_on:
      - spark-master
    ports:
      - 8081:8081
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_WEBUI_PORT: 8081
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive
      POSTGRES_URL: postgres:5432
      DATANUCLEUS_AUTO_CREATE_TABLES: true

  spark-worker-2:
    image: ghcr.io/kbase/cdm-spark-standalone:pr-29
    platform: linux/amd64
    depends_on:
      - spark-master
    ports:
      - 8082:8082
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_WEBUI_PORT: 8082
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive
      POSTGRES_URL: postgres:5432
      DATANUCLEUS_AUTO_CREATE_TABLES: true

  cdm_task_service:
    image: ghcr.io/kbase/cdm-task-service:pr-324
    platform: linux/amd64
    ports:
      - 5000:5000
    depends_on:
      mongodb:
        condition: service_started
      minio-create-bucket:
        condition: service_completed_successfully
      kafka:
        condition: service_started
      auth2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/"]
      interval: 1s
      retries: 5
    environment:
      KBCTS_KBASE_AUTH2_URL: http://auth2:8080/testmode/
      KBCTS_KBASE_AUTH2_ADMIN_ROLES: CDM_TASK_SERVICE_ADMIN
      KBCTS_KBASE_STAFF_ROLE: KBASE_STAFF
      KBCTS_HAS_NERSC_ACCOUNT_ROLE: HAS_NERSC_ACCOUNT
      KBCTS_NERSC_JAWS_USER: fake
      KBCTS_NERSC_JAWS_REFDATA_DIR: fake
      KBCTS_SFAPI_CRED_PATH: fake
      KBCTS_NERSC_REMOTE_CODE_DIR: fake
      KBCTS_JAWS_URL: fake
      KBCTS_JAWS_TOKEN: fake
      KBCTS_JAWS_GROUP: fake
      KBCTS_S3_URL: http://minio:9000
      KBCTS_S3_EXTERNAL_URL: http://minio:9000
      KBCTS_VERIFY_S3_EXTERNAL_URL: false
      KBCTS_S3_ACCESS_KEY: cts
      KBCTS_S3_ACCESS_SECRET: ctspassword
      KBCTS_S3_ALLOW_INSECURE: true
      KBCTS_MONGO_HOST: mongodb://mongodb:27017
      KBCTS_MONGO_DB: cdmtaskservice
      # TODO MONGOAUTH need to add a user to the cdmtaskservice db before this works out
      #                of the box.
      #                The env vars in mongodb below only create a user in admin and only if
      #                the data directory is empty.
      # KBCTS_MONGO_USER: root
      # KBCTS_MONGO_PWD: secret
      KBCTS_MONGO_RETRYWRITES: false
      KBCTS_JOB_MAX_CPU_HOURS: 200
      KBCTS_KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KBCTS_KAFKA_TOPIC_JOBS: cts-jobs
      KBCTS_CONTAINER_S3_LOG_DIR: cts-logs/container_logs
      KBCTS_SERVICE_ROOT_URL: https://superduperfake.com
      KBCTS_SERVICE_GROUP: fake
      KBCTS_SKIP_NERSC: true
    volumes:
      - ./docker_compose/cts_helpers:/cts_helpers

  cdm_events:
    build:
      context: .
      dockerfile: Dockerfile
    platform: linux/amd64
    depends_on:
      postgres:
        condition: service_started
      cdm_task_service:
        condition: service_healthy
      create_auth2_user:
        condition: service_completed_successfully
      kafka:
        condition: service_started
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
    environment:
      CSEP_KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      CSEP_KAFKA_TOPIC_JOBS:      cts-jobs     
      CSEP_KAFKA_TOPIC_JOBS_DLQ: cts-jobs-dlq
      CSEP_KAFKA_GROUP_ID: cts_event_processors
      # If a consumer doesn't poll in this interval the broker will treat it as dead
      CSEP_KAFKA_MAX_POLL_INTERVAL_MS: 7200000
      CSEP_CDM_TASK_SERVICE_URL: http://cdm_task_service:5000
      # To configure an admin token directly use this env var
      # CSEP_CDM_TASK_SERVICE_ADMIN_TOKEN: token_here
      CSEP_CDM_TASK_SERVICE_ADMIN_TOKEN_FILE:        /shared/token.txt    
      SPARK_MASTER_URL: spark://spark-master:7077
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
      POSTGRES_DB: hive
      POSTGRES_URL: postgres:5432
      DATANUCLEUS_AUTO_CREATE_TABLES: true
    # TODO SPARK need to config hive template https://github.com/kbase/cdm-spark-standalone/blob/main/scripts/setup.sh#L56-L61
    volumes:
      - shared-token:/shared
      - ./test/manual:/csep/test/manual
